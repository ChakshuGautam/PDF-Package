{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import html\n",
    "import json\n",
    "import traceback\n",
    "import pdfkit\n",
    "import gspread\n",
    "from gspread.exceptions import SpreadsheetNotFound\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/params.json', 'r') as f:\n",
    "    params = json.loads(f.read())\n",
    "\n",
    "with open('config.json') as json_file:\n",
    "    CONFIG = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fetch_data and get_tags\n",
    "\n",
    "def get_tags(raw_data=None):\n",
    "    \"\"\"\n",
    "    this method return all the tags on the basis of which we filter the request\n",
    "    \"\"\"\n",
    "    tags = dict()\n",
    "    if raw_data:\n",
    "        tags[\"FORMID\"] = raw_data[\"FORMID\"]\n",
    "        tags[\"USERNAME\"] = raw_data[\"USERNAME\"]\n",
    "        tags[\"FORMSUBMISSIONDATE\"] = raw_data[\"FORMSUBMISSIONDATE\"]\n",
    "        tags[\"INSTANCEID\"] = raw_data[\"INSTANCEID\"]\n",
    "        tags[\"FORMNAME\"] = CONFIG[raw_data[\"FORMID\"]][\"FORMNAME\"]\n",
    "        CONFIG[\"SHEETID\"] = CONFIG[raw_data[\"FORMID\"]][\"SHEETID\"]\n",
    "        CONFIG[\"DOCTEMPLATEID\"] = CONFIG[raw_data[\"FORMID\"]][\"DOCTEMPLATEID\"]\n",
    "        CONFIG[\"APPLICATIONID\"] = CONFIG[raw_data[\"FORMID\"]][\"APPLICATIONID\"]\n",
    "        CONFIG['FORMNAME'] = tags[\"FORMNAME\"]\n",
    "        if 'FILENAMEFIELD' in CONFIG[raw_data[\"FORMID\"]]:\n",
    "            CONFIG['FILENAMEFIELD'] = CONFIG[raw_data[\"FORMID\"]][\"FILENAMEFIELD\"]\n",
    "    return tags\n",
    "\n",
    "\n",
    "def fetch_data(req_data=\"\"):\n",
    "        form_id = req_data['formId']\n",
    "        new_req_data = req_data['data'][0]  # Getting the data : [{values}]\n",
    "        instance_id = new_req_data['instanceID']  # Getting the instance id for searching routes\n",
    "        user_name_field = CONFIG[form_id][\"USERNAMEFIELD\"]\n",
    "        new_req_data = json.loads(json.dumps(new_req_data))  # Getting the new data\n",
    "        user_name = new_req_data[user_name_field]\n",
    "        form_submission_date = new_req_data[\n",
    "            '*meta-submission-date*']  # Correcting the submission date and removing the time\n",
    "        end_index = form_submission_date.find(str('T'))\n",
    "        form_submission_date = form_submission_date[:end_index]\n",
    "        # Saving the corrected date in the json\n",
    "        new_req_data['*meta-submission-date*'] = form_submission_date\n",
    "        my_dict = {}\n",
    "        for req_key, req_val in new_req_data.items():\n",
    "            if isinstance(req_val, dict):\n",
    "                for col_key, col_val in req_val.items():\n",
    "                    if col_key == \"url\":\n",
    "                        # correcting the URLs\n",
    "                        base_url = 'http://aggregate.cttsamagra.xyz:8080/'\n",
    "                        index_start = 0  # Finding the substring\n",
    "                        index_end = col_val.find(\n",
    "                            \":8080/\") + 6  # Find the stopping point\n",
    "                        newv1 = col_val.replace(col_val[index_start:index_end], base_url)\n",
    "                        my_dict[req_key] = newv1\n",
    "            elif isinstance(req_val, (float, int)):\n",
    "                my_dict[req_key] = str(req_val)\n",
    "            elif isinstance(req_val, list):\n",
    "                my_dict[req_key] = str(req_val[0])  # Converting list to str\n",
    "            else:\n",
    "                if req_val is None:\n",
    "                    req_val = \"-\"\n",
    "                my_dict[req_key] = req_val\n",
    "\n",
    "        # Calculate Udise from its database and then Calculate distance from udise\n",
    "        calculated_distance = 'Not available'  # Calculate using udise\n",
    "        my_dict['calculated_distance'] = calculated_distance\n",
    "        all_data = dict()\n",
    "        all_data['req_data'] = my_dict\n",
    "        all_data['FORMID'] = form_id\n",
    "        all_data['INSTANCEID'] = instance_id\n",
    "        all_data['USERNAME'] = user_name\n",
    "        all_data['FORMSUBMISSIONDATE'] = form_submission_date\n",
    "        all_data[\"SHEETID\"] = CONFIG[form_id][\"SHEETID\"]\n",
    "        \n",
    "        all_data[\"DOCTEMPLATEID\"] = CONFIG[form_id][\"DOCTEMPLATEID\"]\n",
    "        all_data[\"APPLICATIONID\"] = CONFIG[form_id][\"APPLICATIONID\"]\n",
    "        tags = get_tags(all_data)\n",
    "        all_data.update(CONFIG)\n",
    "        raw_data = dict()\n",
    "        raw_data['reqd_data'] = all_data\n",
    "        raw_data['tags'] = tags\n",
    "        raw_data['instance_id'] = instance_id\n",
    "        if 'DOCDELETED' in CONFIG[all_data[\"FORMID\"]]:\n",
    "            raw_data['is_delete'] = CONFIG[all_data[\"FORMID\"]][\"DOCDELETED\"]\n",
    "        else:\n",
    "            raw_data['is_delete'] = True\n",
    "        return raw_data, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HELLOO!!!\n"
     ]
    }
   ],
   "source": [
    "raw_data = fetch_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('data/raw_data.json', 'w') as f):\n",
    "    f.write(json.dumps(raw_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16IXRddw912l0zQ31EHPnmnm6tPG-O2US5r3LHO_h4IM mappingDetails\n",
      "<gspread.client.Client object at 0x111afb5b0>\n",
      "<gspread.client.Client object at 0x1108909d0>\n"
     ]
    }
   ],
   "source": [
    "### get_mapping values, build_pdf\n",
    "with(open('data/raw_data.json', 'r') as f):\n",
    "    raw_data = json.loads(f.read())\n",
    "form_id = raw_data[0][\"reqd_data\"][\"FORMID\"]\n",
    "config = CONFIG[form_id]\n",
    "\n",
    "def _get_token():\n",
    "    \"\"\" The file token.pickle stores the user's access and refresh tokens, and is\n",
    "        created automatically when the authorization flow completes for the first\n",
    "        time.\"\"\"\n",
    "    client = None\n",
    "    creds = None\n",
    "    try:\n",
    "        sheet_scopes = [\n",
    "            'https://spreadsheets.google.com/feeds',\n",
    "            'https://www.googleapis.com/auth/spreadsheets',\n",
    "            'https://www.googleapis.com/auth/drive'\n",
    "        ]\n",
    "        # base_path = os.path.dirname(__file__)\n",
    "        creds = ServiceAccountCredentials.from_json_keyfile_name('gcs-creds-af.json')\n",
    "        client = gspread.authorize(creds)\n",
    "    except Exception as ex:\n",
    "        print(traceback.format_exc())\n",
    "    return client, creds\n",
    "\n",
    "def get_sheetvalues(sheet_id, var_mapping):\n",
    "    \"\"\"\n",
    "    get google sheet data of the specified sheet id and range\n",
    "    \"\"\"\n",
    "    error = None\n",
    "    try:\n",
    "        client = _get_token()[0]\n",
    "        print(client)\n",
    "        base_sheet = client.open_by_key(sheet_id)\n",
    "        sheet = base_sheet.worksheet(var_mapping)\n",
    "        values = sheet.get_all_values()\n",
    "        # print(values)\n",
    "        if not values:\n",
    "            error = \"No Mapping details found\"\n",
    "        else:\n",
    "            mapping_values = values\n",
    "    except SpreadsheetNotFound as ex:\n",
    "        error = \"Failed to fetch mapping detials - 1\"\n",
    "        mapping_values = None\n",
    "        print(traceback.format_exc())\n",
    "    except Exception as ex:\n",
    "        print(traceback.format_exc())\n",
    "        error = \"Failed to fetch mapping detials - 2\"\n",
    "        mapping_values = None\n",
    "        print(traceback.format_exc())\n",
    "    return mapping_values, error\n",
    "\n",
    "\n",
    "\n",
    "def fetch_mapping(data):\n",
    "    \"\"\"\n",
    "    this method fetches mapping values and options from google sheet and update this in raw_data\n",
    "    return it as raw_data\n",
    "    \"\"\"\n",
    "    error = None\n",
    "    # raw_data = None\n",
    "    try:\n",
    "        print(data['SHEETID'], data['MAPPINGDETAILS'])\n",
    "        get_value_mapping = get_sheetvalues(data['SHEETID'], data['MAPPINGDETAILS'])\n",
    "        mapping_error = get_value_mapping[1]  # Error in fetching mapping\n",
    "        mapping_values = get_value_mapping[0]  # mapping values list\n",
    "        get_options_mapping = get_sheetvalues(data['SHEETID'],\n",
    "                                                    data['OPTIONSSHEET'])\n",
    "        options_error = get_options_mapping[1]  # Error in fetching options\n",
    "        options_mapping = get_options_mapping[0]  # options mapping list\n",
    "\n",
    "        if not mapping_error and not options_error:\n",
    "            raw_data = dict()\n",
    "            raw_data['value_mapping'] = mapping_values\n",
    "            raw_data['options_mapping'] = options_mapping\n",
    "            data.update(raw_data)\n",
    "            raw_data = data\n",
    "\n",
    "        else:\n",
    "            error = str(mapping_error) + str(options_error)\n",
    "\n",
    "    except Exception as ex:\n",
    "        error = \"Failed to fetch mapping detials - 4\"\n",
    "    return raw_data, error\n",
    "\n",
    "raw_data = fetch_mapping(raw_data[0][\"reqd_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('data/raw_data_with_mapping.json', 'w') as f):\n",
    "    f.write(json.dumps(raw_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['shalu', '9355170004', 'Rohtak', '1992-07-27', 'Female', 'Diploma After 12th', '-', '70', '-', '-', '-', 'ccm', 'maruti', '2015-05-01', '2017-05-01', 'teaching', 'receptionist', 'back office/dat entry', 'account', 'Yes', 'Yes', 'Yes', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "def map_data(all_data, mapping_values, options_mapping):\n",
    "    error = None\n",
    "    final_data = None\n",
    "    try:\n",
    "        # info_log(self.logger.info, \"Step4.1 Mapping Start\", self.raw_data)\n",
    "        final_data = []  # List to hold the final values\n",
    "        mapping_values.pop(0)\n",
    "        options_mapping.pop(0)\n",
    "        for row in mapping_values:\n",
    "            if row[1].lower() == 'options':\n",
    "                options_mapping_keys = [x[0] for x in options_mapping]\n",
    "                option_value_start = options_mapping_keys.index(row[2])\n",
    "                if option_value_start == -1:\n",
    "                    all_data[row[2]] = 'NO_TEXT_FOUND'  # If the particular option is not found\n",
    "                    final_data.append(all_data[row[2]])\n",
    "                else:\n",
    "                    a = options_mapping_keys.index(row[2])\n",
    "                    current_option_val = 'NO_TEXT_FOUND'\n",
    "                    for i in options_mapping[a][1:]:\n",
    "                        if i != '':\n",
    "                            op_key = i.split(\"::\")[0]\n",
    "                            op_val = i.split(\"::\")[1]\n",
    "                            if op_key == all_data[row[2]]:\n",
    "                                current_option_val = op_val\n",
    "                                break\n",
    "                    final_data.append(current_option_val)\n",
    "            else:\n",
    "                if not all_data[row[2]]:\n",
    "                    all_data[row[2]] = 'NO_TEXT_FOUND'  # If data is None\n",
    "\n",
    "                final_data.append(all_data[row[2]])  # Appending the received data to the final list\n",
    "        # info_log(self.logger.info, \"Step4.1 Mapping End\", self.raw_data)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(traceback.format_exc())\n",
    "        error = \"Failed to map data\"\n",
    "        # info_log(self.logger.error, \"Error3 \" + error, self.raw_data)\n",
    "        # self.logger.error(\"Exception occurred\", exc_info=True)\n",
    "    return final_data, error\n",
    "\n",
    "def build_pdf(raw_data, file_name):\n",
    "    \"\"\"\n",
    "    this method get raw_data and file name and generate pdf having this file_name\n",
    "    \"\"\"\n",
    "    error = None\n",
    "    pdf_name = ''\n",
    "    pdf_url = ''\n",
    "    try:\n",
    "        data = raw_data['req_data']\n",
    "        mapping_values = raw_data['value_mapping']\n",
    "        options_mapping = raw_data['options_mapping']\n",
    "        mapped_data = map_data(data, mapping_values, options_mapping)\n",
    "        return mapped_data\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "with(open('data/raw_data_with_mapping.json', 'r') as f):\n",
    "    raw_data_with_mapping = json.loads(f.read())\n",
    "\n",
    "final_map = build_pdf(raw_data_with_mapping[0], \"out.pdf\")\n",
    "mapping_error = final_map[1]\n",
    "if not mapping_error:\n",
    "    mapped_data = final_map[0]\n",
    "print(mapped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/template.html', 'rb') as f:\n",
    "    html_content = html.unescape(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(mapped_data):\n",
    "    template_index = idx + 1\n",
    "    string_to_search = '<<' + str(template_index) + '>>'\n",
    "    html_content = html_content.replace(string_to_search, val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading pages (1/6)\nCounting pages (2/6)                                               \nResolving links (4/6)                                                       \nLoading headers and footers (5/6)                                           \nPrinting pages (6/6)\nDone                                                                      \n"
     ]
    }
   ],
   "source": [
    "with(open('data/filled_template.html', 'w') as f):\n",
    "    f.write(html_content)\n",
    "\n",
    "with open('data/filled_template.html') as f:    \n",
    "    pdfkit.from_file(f, 'data/out.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}